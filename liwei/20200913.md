- date: 20200913 
- author: liwei

## Plan

- [x] the relationship between tensorflow and tf lite
- [ ] tensorflow quantization

## Notes

### The relationship between tf and tf lite

一直听闻 tf lite 和 tf 有一些区别，今天打算多方面比较一下。

定位上：
tensorflow lite 的描述是一个用于部署 machine learning 模型到移动以及 iot 设备进行推理的深框架学习。
tensorflow 则是号称一个 end-to-end 的开源机器学习平台。

工作流上：
tensorflow lite：选择模型 -> 转换模型 -> 部署 -> 优化
tensorflow：构建模型 -> 训练模型 -> 选择平台 -> 部署 -> 优化

整体上来看，tf lite 实际上就是一个轻量化的 inference engine，其目标是方便的提供用户在低算力平台上运行 AI 的能力。他和 tensorflow 的关系，与 paddlepaddle 与 anakin 的关系类似。

### tensorflow quantization

对于 tf 有两种量化方式，post-training quantization 和 training aware quantization。顾名思义，前者是在训练后，针对高精度模型直接进行量化推理，这种方式使用更为简单；后者则是训练的过程中，就进行量化，这种量化方式虽然通常精度更高，但更为复杂，对算法研究人员的要求更高。

#### training aware quantization

训练可感知量化的内核在于，在训练时模拟量化的行为过程，训练出来的模型，可以直接让后续的业务进行量化推理。量化推理的收益主要体现在两点：1. 模型变小，内存占用减少；2. 速度加快，延迟降低；在量化训练的过程中，用户可以自定义一些量化参数，例如量化位宽等。但是目前 tensorflow 尚未承诺量化训练相关的 api 是稳定兼容的。
可以参考的资料是 [QAT](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html) 介绍 blog。

QAT 的核心思想，是在训练时，在正向计算的过程中模拟低精度量化的行为。谷歌号称自己的移动视觉团队最先提出了这个想法，其原理在于量化带来的误差，同样可以作为一个训练过程中的噪声，纳入到全局的损失中，而训练的过程就是降低这一损失。在实际操作中，针对需要量化的 op ，其输入的 tensor 会经过一个 fakequant op，这个 op 完成了 float-point 数据到 fixed-point 数据的转换；而在需要量化计算的 op 计算完成后，会有同样一个反量化 op 将量化 tensor 转回到高精度 tensor。这个过程，假设所有的误差损失都在 fakequant op 中发生，而量化 op 计算没有误差损失。

tensorflow lite 的量化 spec 参考链接是 [tf lite quant spec](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fake-quant-with-min-max-vars)。

#### post-training quantization

## More

https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide